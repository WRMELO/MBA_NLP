
# üó∫Ô∏è **Resumo Detalhado ‚Äî Pipeline NLP at√© Etapa 2.2**

Este documento registra de forma clara e rastre√°vel **todas as etapas j√° conclu√≠das** no projeto de classifica√ß√£o de chamados, seguindo o **Plano de A√ß√£o Detalhado** e o **PROTOCOLO LLM UNIVERSAL v5.2**.

---

## ‚úÖ **1Ô∏è‚É£ Setup de Infraestrutura**

**Objetivo:** Garantir que o ambiente tenha todas as bibliotecas, modelos de linguagem e vari√°veis de caminho necess√°rios para execu√ß√£o reproduc√≠vel.

**A√ß√µes realizadas:**
- **Instala√ß√£o e verifica√ß√£o de bibliotecas:** `pandas`, `numpy`, `scikit-learn`, `nltk`, `spacy`, `unidecode`, `tqdm` e `sentence-transformers` foram validados ou instalados.
- **Download do modelo SpaCy `pt_core_news_sm`:** confirmamos que o modelo est√° dispon√≠vel para tarefas de tokeniza√ß√£o e lematiza√ß√£o em portugu√™s.
- **Instala√ß√£o de `punkt` e `stopwords` do NLTK:** tentativa de uso para tokeniza√ß√£o inicial, alinhado √†s pr√°ticas de introdu√ß√£o.
- **Defini√ß√£o de constantes globais:** `URL_ORIGINAL`, `BASE_DIR` e `PATH_RAW` configurados para rastrear a origem dos dados (`S3`) e o armazenamento local no Google Drive (`/MBA_NLP/bases_criadas`).

---

## ‚úÖ **2Ô∏è‚É£ EDA Inicial**

**Objetivo:** Entender a estrutura do dataset, verificar nulos, analisar distribui√ß√£o de textos e categorias.

**A√ß√µes realizadas:**
- **Carregamento do dataset original:** `dados_originais.csv` foi carregado de forma validada via `PATH_RAW`.
- **Explora√ß√£o de metadados:** Usamos `df.info()`, `df.shape` e `df.head(20)` para verificar consist√™ncia.
- **An√°lise de valores nulos:** Percentuais verificados ‚Äî sem inconsist√™ncias graves.
- **An√°lise de comprimento dos textos:** Histogramas indicaram varia√ß√£o de textos de 20 a 2.000+ caracteres.
- **Balanceamento de classes:** `value_counts()` + gr√°fico de barras revelaram leve desbalanceamento entre categorias como `Servi√ßos de Conta Banc√°ria` e `Outros`.

---

## ‚úÖ **3Ô∏è‚É£ Limpeza de Texto ‚Äî Primeira Fase**

**Objetivo:** Remover ru√≠dos e anonimizar dados sens√≠veis presentes em `descricao_reclamacao`.

**A√ß√µes realizadas:**
- **Fun√ß√£o `clean_text()`:** Aplicou lowercase, remo√ß√£o de acentos (`unidecode`), pontua√ß√£o e espa√ßos extras.
- **Identifica√ß√£o de `xxxx`:** Reconhecemos que os placeholders representam informa√ß√µes sens√≠veis como PII, datas e IDs.
- **Fun√ß√£o `replace_xxxx_tokens()`:** Substitu√≠mos padr√µes `xxxx` por tokens sem√¢nticos `<PII>`, `<DATE>`, `<ID>` ou `<UNK>`.
- **Salvamento da vers√£o intermedi√°ria:** `dados_com_tokens.csv` armazenado em `/bases_criadas`.

---

## ‚úÖ **4Ô∏è‚É£ Tokeniza√ß√£o ‚Äî Estrat√©gia Revisada**

**Contexto:** A abordagem inicial usava `NLTK` com `punkt`. Devido a falhas recorrentes no ambiente Colab (bug `punkt_tab not found`), foi decidida a **troca metodol√≥gica**:

- **Fontes acad√™micas e materiais de aula recomendam SpaCy** para tokeniza√ß√£o em portugu√™s, pela robustez do modelo `pt_core_news_sm`.
- A tokeniza√ß√£o SpaCy √© lingu√≠stica, com suporte nativo ao idioma, incluindo POS-Tagging e lematiza√ß√£o.
- Adicionamos o filtro `token.is_alpha` para extrair apenas palavras v√°lidas, alinhado aos exemplos `Aula_1_DTS_PLN_Exerc√≠cios`.

**Resultado:**  
- A coluna `texto_tokens_list` foi criada com tokens extra√≠dos do SpaCy.
- Utilizamos `tqdm` integrado (`tqdm.pandas()`) para monitorar o progresso da tokeniza√ß√£o, refor√ßando rastreabilidade acad√™mica.
- O arquivo final `dados_tokens_tokenized.csv` foi salvo na pasta `/bases_criadas`.

---

## üóÉÔ∏è **5Ô∏è‚É£ Estrat√©gia com NotebookLM**

**Decis√£o metodol√≥gica:**  
Para **acompanhar, validar e documentar** cada etapa do fluxo, optamos por:
- **Executar o pipeline no Google Colab**, integrando o Google Drive para persist√™ncia.
- **Registrar cada c√©lula explicativa em Markdown**, no padr√£o exigido pelo `PROTOCOLO LLM UNIVERSAL v5.2`.
- **Validar itera√ß√µes complexas (como a substitui√ß√£o de placeholders e refinamentos lingu√≠sticos)** usando o **NotebookLM**, que atua como reposit√≥rio de experimentos e como ponte de controle para feedback de boas pr√°ticas.

Assim, garantimos **rastreabilidade total**, versionamento dos dados (`dados_originais.csv`, `dados_com_tokens.csv`, `dados_tokens_tokenized.csv`) e alinhamento com a trilha acad√™mica.

---

## üö© **Resumo do Progresso**

| Macro-Bloco | Status |
|-------------|--------|
| Setup Infraestrutura | ‚úÖ Conclu√≠do |
| EDA Inicial | ‚úÖ Conclu√≠do |
| Limpeza de Texto | ‚úÖ Fase Base Conclu√≠da |
| Tokeniza√ß√£o | ‚úÖ Conclu√≠da com SpaCy + `tqdm` |
| Stopwords | ‚è≠Ô∏è Pr√≥xima etapa |

---

### üìå **Pr√≥ximo Passo**

A seguir, avan√ßaremos para a **Etapa 2.3 ‚Äî Filtro de Stopwords (pt)**, aplicando a mesma robustez t√©cnica, combinando NLTK e SpaCy, para manter o texto mais limpo para vetoriza√ß√£o.

---
