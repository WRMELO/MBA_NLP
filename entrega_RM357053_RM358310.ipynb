{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJiFwDqtf6eE"
      },
      "source": [
        "# **Case QuantumFinance - Disciplina NLP - Classificador de chamados**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXvR5XhTOG0O"
      },
      "source": [
        "**Aten√ß√£o:**\n",
        "- Leia com aten√ß√£o o descritivo do trabalho e as orienta√ß√µes do template.\n",
        "- O trabalho deve ser entregue respeitando a estrutura do arquivo de template em notebook \"Template_Trabalho_Final_NLP.ipynb\" e compactado no formato .zip. Apenas um arquivo no formato .ipynb deve ser entregue consolidando todo o trabalho."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDbi6PDS9MYO"
      },
      "source": [
        "***Participantes (RM - NOME):***<br>\n",
        "# RM 357053 - WILSON ROBERTO DE MELO\n",
        "# RM 358310 - RAFAEL DE MIRANDA MEIRELLES COSTA E SILVA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xw6WhaNo4k3"
      },
      "source": [
        "### **Crie um classificador de chamados aplicando t√©cnicas de PLN**\n",
        "---\n",
        "\n",
        "A **QuantumFinance** tem um canal de atendimento via chat e precisar classificar os assuntos dos atendimentos para melhorar as tratativas dos chamados dos clientes. O canal recebe textos abertos dos clientes relatando o problema e/ou d√∫vida e depois √© direcionado para alguma √°rea especialista no assunto para uma melhor tratativa.‚Äã\n",
        "\n",
        "1. Crie ao menos um modelo classificador de assuntos aplicando t√©cnicas de NLP (PLN), Vetoriza√ß√£o (n-grama + m√©trica) e modelo supervisionado, que consiga classificar atrav√©s de um texto o assunto conforme dispon√≠vel na base de dados [1] para treinamento e valida√ß√£o do seu modelo.‚Äã\n",
        "\n",
        "  O modelo precisar atingir um score na **m√©trica F1 Score superior a 75%**. Utilize o dataset [1] para treinar e testar o modelo, separe o dataset em duas amostras (75% para treinamento e 25% para teste com o randon_state igual a 42).‚Äã\n",
        "\n",
        "2. Utilizar ao menos uma aplica√ß√£o de modelos com Embeddings usando Word2Vec e/ou LLM¬¥s para criar o modelo classificador com os crit√©rios do item 1. N√£o √© necess√°rio implementar aplica√ß√µes usando servi√ßos de API da OpenAI ou outros por exemplo.\n",
        "\n",
        "Fique √† vontade para testar e explorar as t√©cnicas de pr√©-processamento, abordagens de NLP, algoritmos e bibliotecas, mas explique e justifique as suas decis√µes durante o desenvolvimento.‚Äã\n",
        "\n",
        "**Composi√ß√£o da nota:‚Äã**\n",
        "\n",
        "**50%** - Demonstra√ß√µes das aplica√ß√µes das t√©cnicas de PLN (regras, pr√©-processamentos, tratamentos, variedade de modelos aplicados, aplica√ß√µes de GenIA, organiza√ß√£o do pipeline, etc.)‚Äã\n",
        "\n",
        "**50%** - Baseado na performance (score) obtida com a amostra de teste no pipeline do modelo campe√£o (validar com  a M√©trica F1 Score). **Separar o pipeline completo do modelo campe√£o conforme template.‚Äã**\n",
        "\n",
        "O trabalho poder√° ser feito em grupo de 2 at√© 4 pessoas (mesmo grupo do Startup One) e trabalhos iguais ser√£o descontado nota e pass√≠vel de reprova√ß√£o.\n",
        "\n",
        "**[1] = ‚Äãhttps://dados-ml-pln.s3.sa-east-1.amazonaws.com/tickets_reclamacoes_classificados.csv**\n",
        "\n",
        "**[F1 Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)** com average='weighted'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyKC9Vhkp0BK"
      },
      "source": [
        "Bom desenvolvimento!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlxCSMk-iAdk"
      },
      "source": [
        "###**Area de desenvolvimento e valida√ß√µes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O5PedDdiZLb"
      },
      "source": [
        "Fa√ßa aqui as demonstra√ß√µes das aplica√ß√µes das t√©cnicas de PLN (regras, pr√©-processamentos, tratamentos, variedade de modelos aplicados, organiza√ß√£o do pipeline, etc.)‚Äã\n",
        "\n",
        "Fique √† vontade para testar e explorar as t√©cnicas de pr√©-processamento, abordagens de NLP, algoritmos e bibliotecas, mas explique e justifique as suas decis√µes durante o desenvolvimento.‚Äã"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "---\n",
        "---\n",
        "TODO O DESENVOLVIMENTO FOI FEITO EM COLAB, CONECTADO EM L4\n",
        "---\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Abaixo est√£o c√©lulas, c√≥pias do notebook \"nlp_entrega.ipynb\" usado para o desenvolvimento e compara√ß√£o entre as metofologias, estando com os paths adequados √†quilo que foi desenvolvido, tanto no GD e no SSD local.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FziwgqJmw9OD"
      },
      "outputs": [],
      "source": [
        " üîß ETAPA: MONTAGEM DO GOOGLE DRIVE E SALVAMENTO DO CSV NO DIRET√ìRIO BASE\n",
        "\n",
        "# 1Ô∏è‚É£ Monta o Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2Ô∏è‚É£ Importa pandas\n",
        "import pandas as pd\n",
        "\n",
        "# 3Ô∏è‚É£ Carrega o CSV remoto com fallback para separador\n",
        "url = 'https://dados-ml-pln.s3.sa-east-1.amazonaws.com/tickets_reclamacoes_classificados.csv'\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(url, sep=None, engine='python')\n",
        "except Exception:\n",
        "    df = pd.read_csv(url, sep=';', engine='python')\n",
        "\n",
        "# 4Ô∏è‚É£ Verifica estrutura\n",
        "print(df.info())\n",
        "print(df.head(20))\n",
        "\n",
        "# 5Ô∏è‚É£ Salva no diret√≥rio mostrado na imagem\n",
        "output_path = '/content/drive/MyDrive/MBA_NLP/bases_criadas/dados_originais.csv'\n",
        "df.to_csv(output_path, index=False, encoding='utf-8')\n",
        "\n",
        "print(f'\\n‚úÖ CSV salvo em: {output_path}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß ETAPA: VERIFICA√á√ÉO E INSTALA√á√ÉO DE BIBLIOTECAS ESSENCIAIS\n",
        "\n",
        "import sys\n",
        "\n",
        "# Lista de bibliotecas obrigat√≥rias\n",
        "required_packages = [\n",
        "    'pandas', 'numpy', 'scikit-learn',\n",
        "    'nltk', 'spacy', 'unidecode',\n",
        "    'tqdm', 'sentence-transformers'\n",
        "]\n",
        "\n",
        "# Flag para controlar se precisa rodar o download do modelo SpaCy\n",
        "need_spacy = False\n",
        "\n",
        "# Instala cada pacote se n√£o estiver presente\n",
        "for package in required_packages:\n",
        "    try:\n",
        "        __import__(package.replace('-', '_'))\n",
        "        print(f'‚úÖ {package} OK')\n",
        "    except ImportError:\n",
        "        print(f'‚öôÔ∏è Instalando {package} ...')\n",
        "        !{sys.executable} -m pip install {package}\n",
        "        if package == 'spacy':\n",
        "            need_spacy = True\n",
        "\n",
        "# Download do modelo SpaCy pt_core_news_sm se necess√°rio\n",
        "if need_spacy:\n",
        "    print(\"‚öôÔ∏è Baixando modelo SpaCy pt_core_news_sm ...\")\n",
        "    !python -m spacy download pt_core_news_sm\n",
        "else:\n",
        "    print(\"‚úÖ Verificando se modelo SpaCy j√° existe ...\")\n",
        "    try:\n",
        "        import spacy\n",
        "        spacy.load(\"pt_core_news_sm\")\n",
        "        print(\"‚úÖ Modelo SpaCy pt_core_news_sm j√° est√° dispon√≠vel.\")\n",
        "    except:\n",
        "        print(\"‚öôÔ∏è Baixando modelo SpaCy pt_core_news_sm ...\")\n",
        "        !python -m spacy download pt_core_news_sm\n",
        "\n",
        "print(\"\\n Verifica√ß√£o conclu√≠da.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß ETAPA: DOWNLOAD E VALIDA√á√ÉO DO MODELO SPACY pt_core_news_sm\n",
        "\n",
        "import spacy\n",
        "\n",
        "try:\n",
        "    nlp = spacy.load('pt_core_news_sm')\n",
        "    print(\"‚úÖ Modelo SpaCy `pt_core_news_sm` j√° est√° instalado e carregado.\")\n",
        "except OSError:\n",
        "    print(\"‚öôÔ∏è Baixando modelo SpaCy `pt_core_news_sm` ...\")\n",
        "    !python -m spacy download pt_core_news_sm\n",
        "    nlp = spacy.load('pt_core_news_sm')\n",
        "    print(\"‚úÖ Download conclu√≠do e modelo carregado.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß ETAPA: INSTALA√á√ÉO DO TOKENIZER NLTK E STOPWORDS\n",
        "\n",
        "import nltk\n",
        "\n",
        "# Download dos recursos necess√°rios\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "print(\"‚úÖ Tokenizer `punkt` e stopwords em portugu√™s prontos para uso.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß ETAPA: DEFINI√á√ÉO DE CONSTANTES DE CAMINHO GLOBAIS\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# URL original do dataset (caso precise baixar novamente)\n",
        "URL_ORIGINAL = 'https://dados-ml-pln.s3.sa-east-1.amazonaws.com/tickets_reclamacoes_classificados.csv'\n",
        "\n",
        "# Caminho base no Google Drive (ajuste conforme sua estrutura)\n",
        "BASE_DIR = Path('/content/drive/MyDrive/MBA_NLP/bases_criadas')\n",
        "\n",
        "# Caminho do arquivo CSV original salvo localmente\n",
        "PATH_RAW = BASE_DIR / 'dados_originais.csv'\n",
        "\n",
        "# Valida√ß√£o: o arquivo existe?\n",
        "if PATH_RAW.exists():\n",
        "    print(f'‚úÖ Arquivo encontrado em: {PATH_RAW}')\n",
        "else:\n",
        "    print(f'‚ö†Ô∏è Aten√ß√£o: {PATH_RAW} n√£o encontrado! Verifique o salvamento.')\n",
        "\n",
        "# Caminhos registrados\n",
        "print(f\"\\nüåê URL_ORIGINAL: {URL_ORIGINAL}\\nüìÇ BASE_DIR: {BASE_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß ETAPA: CARREGAMENTO DO DATASET LOCAL E VALIDA√á√ÉO EDA INICIAL\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Carrega o arquivo local validado\n",
        "df = pd.read_csv(PATH_RAW)\n",
        "\n",
        "# Informa√ß√µes gerais\n",
        "print(\"=== Estrutura do DataFrame ===\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\n=== Dimens√£o do DataFrame ===\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "\n",
        "print(\"\\n=== Primeiras 20 linhas ===\")\n",
        "print(df.head(20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß ETAPA: AN√ÅLISE DE NULOS, HISTOGRAMA DE TEXTO E BALANCEAMENTO DE CLASSES\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1Ô∏è‚É£ Percentual de nulos por coluna\n",
        "print(\"=== Percentual de valores nulos por coluna ===\")\n",
        "percent_nulos = df.isnull().mean() * 100\n",
        "print(percent_nulos)\n",
        "\n",
        "# 2Ô∏è‚É£ Histograma do tamanho dos textos de reclama√ß√£o\n",
        "df['text_length'] = df['descricao_reclamacao'].astype(str).apply(len)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.hist(df['text_length'], bins=50, color='skyblue', edgecolor='black')\n",
        "plt.title('Distribui√ß√£o do Tamanho dos Textos de Reclama√ß√£o')\n",
        "plt.xlabel('N√∫mero de Caracteres')\n",
        "plt.ylabel('Frequ√™ncia')\n",
        "plt.show()\n",
        "\n",
        "# 3Ô∏è‚É£ Contagem de classes na coluna-alvo 'categoria'\n",
        "print(\"\\n=== Contagem de amostras por categoria ===\")\n",
        "print(df['categoria'].value_counts())\n",
        "\n",
        "# Gr√°fico de barras das classes\n",
        "df['categoria'].value_counts().plot(kind='bar', figsize=(10,6), color='coral', edgecolor='black')\n",
        "plt.title('Distribui√ß√£o das Categorias de Reclama√ß√£o')\n",
        "plt.xlabel('Categoria')\n",
        "plt.ylabel('Contagem')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 4Ô∏è‚É£ Verifica as primeiras linhas com o campo de tamanho de texto\n",
        "print(\"\\n=== Exemplo com comprimento de texto ===\")\n",
        "print(df[['descricao_reclamacao', 'text_length']].head(20))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "=== Percentual de valores nulos por coluna ===\n",
        "id_reclamacao           0.0\n",
        "data_abertura           0.0\n",
        "categoria               0.0\n",
        "descricao_reclamacao    0.0\n",
        "dtype: float64\n",
        "\n",
        "\n",
        "=== Contagem de amostras por categoria ===\n",
        "categoria\n",
        "Servi√ßos de conta banc√°ria             5161\n",
        "Cart√£o de cr√©dito / Cart√£o pr√©-pago    5006\n",
        "Roubo / Relat√≥rio de disputa           4822\n",
        "Hipotecas / Empr√©stimos                3850\n",
        "Outros                                 2233\n",
        "Name: count, dtype: int64\n",
        "\n",
        "\n",
        "=== Exemplo com comprimento de texto ===\n",
        "                                 descricao_reclamacao  text_length\n",
        "0   Bom dia, meu nome √© xxxx xxxx e agrade√ßo se vo...          505\n",
        "1   Atualizei meu cart√£o xxxx xxxx em xx/xx/2018 e...          350\n",
        "2   O cart√£o Chase foi relatado em xx/xx/2019. No ...          228\n",
        "3   Em xx/xx/2018, enquanto tentava reservar um ti...         1577\n",
        "4   Meu neto me d√™ cheque por {$ 1600,00} Eu depos...          607\n",
        "5                        Voc√™ pode remover a consulta           28\n",
        "6   Sem aviso pr√©vio J.P. Morgan Chase restringiu ...         2409\n",
        "7   Durante os meses de ver√£o, experimento uma ren...         1493\n",
        "8   Em xxxx xx/xx/2019, fiz um pagamento {$ 300.00...         3795\n",
        "9   Eu tenho um cart√£o de cr√©dito Chase que est√° r...          155\n",
        "10     Mishandling desta conta por Chase Auto e XXXX.           46\n",
        "11  Entrei em contato com o XXXX v√°rias vezes na t...          696\n",
        "12  Abri uma conta no Chase Bank no xxxx e usei um...          430\n",
        "13  Para quem possa interessar, o Chase Bank cobro...          654\n",
        "14  Meu cart√£o Chase Amazon foi recusado para uma ...         2205\n",
        "15  Abri a conta de poupan√ßa para o b√¥nus {$ 25,00...          582\n",
        "16  Xxxx xxxx um sof√°, assento de amor, mesa e cad...          186\n",
        "17  Meu cart√£o desapareceu e eu n√£o percebi at√© ho...          203\n",
        "18  Chase me enviou um e -mail hoje com o t√≠tulo i...         1010\n",
        "19  Fiz uma compra com xxxx xxxx xxxx em xx/xx/201...         2087\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß ETAPA: FUN√á√ÉO DE LIMPEZA B√ÅSICA DE TEXTO (Etapa 2.1)\n",
        "\n",
        "import re\n",
        "from unidecode import unidecode\n",
        "\n",
        "# Define fun√ß√£o de limpeza\n",
        "def clean_text(text):\n",
        "    if pd.isnull(text):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = unidecode(text)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove pontua√ß√£o e n√∫meros\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove espa√ßos extras\n",
        "    return text\n",
        "\n",
        "# Aplica no campo original\n",
        "df['texto_limpo'] = df['descricao_reclamacao'].apply(clean_text)\n",
        "\n",
        "# Verifica amostra\n",
        "print(df[['descricao_reclamacao', 'texto_limpo']].head(20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß ETAPA: SUBSTITUI√á√ÉO DE PLACEHOLDERS E SALVAMENTO DO DATAFRAME\n",
        "\n",
        "import re\n",
        "\n",
        "def replace_xxxx_tokens(text):\n",
        "    if pd.isnull(text):\n",
        "        return \"\"\n",
        "\n",
        "    # Substitui datas anonimizadas\n",
        "    text = re.sub(r'\\b(?:x{2}/x{2}/x{2,4})\\b', '<DATE>', text)\n",
        "\n",
        "    # Substitui nomes\n",
        "    text = re.sub(r'\\bnome\\s+e?\\s+x{2,}\\b', '<PII>', text)\n",
        "    text = re.sub(r'\\bnome\\s+x{2,}\\b', '<PII>', text)\n",
        "\n",
        "    # Substitui conta/cart√£o\n",
        "    text = re.sub(r'\\bconta\\s+x{2,}\\b', '<ID>', text)\n",
        "    text = re.sub(r'\\bcartao\\s+x{2,}\\b', '<ID>', text)\n",
        "\n",
        "    # Substitui qualquer xxxx residual\n",
        "    text = re.sub(r'\\b[x]{2,}\\b', '<UNK>', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Aplica no texto j√° limpo\n",
        "df['texto_tokens'] = df['texto_limpo'].apply(replace_xxxx_tokens)\n",
        "\n",
        "# Confere amostra\n",
        "print(df[['descricao_reclamacao', 'texto_limpo', 'texto_tokens']].head(20))\n",
        "\n",
        "# Caminho para salvar no Google Drive\n",
        "path_tokens = BASE_DIR / 'dados_com_tokens.csv'\n",
        "df.to_csv(path_tokens, index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"\\n‚úÖ DataFrame com tokens salvo em: {path_tokens}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß ETAPA: TOKENIZA√á√ÉO DOS TEXTOS COM SPACY E SALVAMENTO ‚Äî COM TQDM\n",
        "\n",
        "import spacy\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Ativa barra de progresso para opera√ß√µes pandas\n",
        "tqdm.pandas()\n",
        "\n",
        "# Carrega modelo SpaCy portugu√™s\n",
        "nlp = spacy.load('pt_core_news_sm')\n",
        "\n",
        "def spacy_tokenizer(text):\n",
        "    doc = nlp(text)\n",
        "    return [token.text for token in doc if token.is_alpha]\n",
        "\n",
        "# Aplica tokeniza√ß√£o com barra de progresso\n",
        "df['texto_tokens_list'] = df['texto_tokens'].progress_apply(spacy_tokenizer)\n",
        "\n",
        "# Exibe amostra\n",
        "print(df[['texto_tokens', 'texto_tokens_list']].head(20))\n",
        "\n",
        "# Salva DataFrame tokenizado\n",
        "path_tokens_tokenized = BASE_DIR / 'dados_tokens_tokenized.csv'\n",
        "df.to_csv(path_tokens_tokenized, index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"\\n‚úÖ DataFrame com tokens tokenizados via SpaCy salvo em: {path_tokens_tokenized}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß ETAPA: REMO√á√ÉO DE STOPWORDS COM LISTA COMBINADA NLTK + SPACY\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Garante download de stopwords NLTK\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Carrega stopwords do NLTK\n",
        "stopwords_nltk = set(stopwords.words('portuguese'))\n",
        "\n",
        "# Carrega stopwords do SpaCy\n",
        "nlp = spacy.load('pt_core_news_sm')\n",
        "stopwords_spacy = nlp.Defaults.stop_words\n",
        "\n",
        "# Combina as listas e remove duplicatas\n",
        "stopwords_combined = stopwords_nltk.union(stopwords_spacy)\n",
        "\n",
        "print(f\"üìå Stopwords NLTK: {len(stopwords_nltk)}\")\n",
        "print(f\"üìå Stopwords SpaCy: {len(stopwords_spacy)}\")\n",
        "print(f\"üìå Stopwords Combinadas (√∫nicas): {len(stopwords_combined)}\")\n",
        "\n",
        "# Ativa barra de progresso para aplica√ß√£o\n",
        "tqdm.pandas()\n",
        "\n",
        "# Fun√ß√£o para filtrar tokens\n",
        "def remove_stopwords(tokens):\n",
        "    return [word for word in tokens if word.lower() not in stopwords_combined]\n",
        "\n",
        "# Aplica remo√ß√£o\n",
        "df['tokens_sem_stopwords'] = df['texto_tokens_list'].progress_apply(remove_stopwords)\n",
        "\n",
        "# Verifica amostra\n",
        "print(df[['texto_tokens_list', 'tokens_sem_stopwords']].head(20))\n",
        "\n",
        "# Salva DataFrame sem stopwords\n",
        "path_tokens_no_stopwords = BASE_DIR / 'dados_tokens_no_stopwords.csv'\n",
        "df.to_csv(path_tokens_no_stopwords, index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"\\n‚úÖ DataFrame sem stopwords salvo em: {path_tokens_no_stopwords}\")\n",
        "# üîß ETAPA: REMO√á√ÉO DE STOPWORDS COM LISTA COMBINADA NLTK + SPACY\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Garante download de stopwords NLTK\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Carrega stopwords do NLTK\n",
        "stopwords_nltk = set(stopwords.words('portuguese'))\n",
        "\n",
        "# Carrega stopwords do SpaCy\n",
        "nlp = spacy.load('pt_core_news_sm')\n",
        "stopwords_spacy = nlp.Defaults.stop_words\n",
        "\n",
        "# Combina as listas e remove duplicatas\n",
        "stopwords_combined = stopwords_nltk.union(stopwords_spacy)\n",
        "\n",
        "print(f\"üìå Stopwords NLTK: {len(stopwords_nltk)}\")\n",
        "print(f\"üìå Stopwords SpaCy: {len(stopwords_spacy)}\")\n",
        "print(f\"üìå Stopwords Combinadas (√∫nicas): {len(stopwords_combined)}\")\n",
        "\n",
        "# Ativa barra de progresso para aplica√ß√£o\n",
        "tqdm.pandas()\n",
        "\n",
        "# Fun√ß√£o para filtrar tokens\n",
        "def remove_stopwords(tokens):\n",
        "    return [word for word in tokens if word.lower() not in stopwords_combined]\n",
        "\n",
        "# Aplica remo√ß√£o\n",
        "df['tokens_sem_stopwords'] = df['texto_tokens_list'].progress_apply(remove_stopwords)\n",
        "\n",
        "# Verifica amostra\n",
        "print(df[['texto_tokens_list', 'tokens_sem_stopwords']].head(20))\n",
        "\n",
        "# Salva DataFrame sem stopwords\n",
        "path_tokens_no_stopwords = BASE_DIR / 'dados_tokens_no_stopwords.csv'\n",
        "df.to_csv(path_tokens_no_stopwords, index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"\\n‚úÖ DataFrame sem stopwords salvo em: {path_tokens_no_stopwords}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß ETAPA: LEMATIZA√á√ÉO COM SPACY E SALVAMENTO\n",
        "\n",
        "import spacy\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Carrega modelo SpaCy portugu√™s\n",
        "nlp = spacy.load('pt_core_news_sm')\n",
        "\n",
        "# Ativa barra de progresso para lematiza√ß√£o\n",
        "tqdm.pandas()\n",
        "\n",
        "def lemmatize_tokens(tokens):\n",
        "    doc = nlp(\" \".join(tokens))\n",
        "    return [token.lemma_ for token in doc if token.is_alpha]\n",
        "\n",
        "# Aplica lematiza√ß√£o\n",
        "df['tokens_lematizados'] = df['tokens_sem_stopwords'].progress_apply(lemmatize_tokens)\n",
        "\n",
        "# Exibe amostra\n",
        "print(df[['tokens_sem_stopwords', 'tokens_lematizados']].head(20))\n",
        "\n",
        "# Salva DataFrame lematizado\n",
        "path_tokens_lematizados = BASE_DIR / 'dados_tokens_lematizados.csv'\n",
        "df.to_csv(path_tokens_lematizados, index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"\\n‚úÖ DataFrame com tokens lematizados salvo em: {path_tokens_lematizados}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìä Macro-Bloco 3 ‚Äî Vetoriza√ß√£o e Engenharia de Features\n",
        "\n",
        "### üéØ **Objetivo**\n",
        "Este bloco foi projetado para criar m√∫ltiplas representa√ß√µes vetoriais de cada reclama√ß√£o da base, usando diferentes abordagens de Processamento de Linguagem Natural (PLN).  \n",
        "Cada abordagem gera uma matriz de features que alimentar√° modelos supervisionados de classifica√ß√£o na etapa seguinte.\n",
        "\n",
        "O prop√≥sito √© **comparar empiricamente** como cada estrat√©gia de vetoriza√ß√£o impacta o desempenho preditivo (com foco em F1 Score weighted ‚â• 75%).  \n",
        "Isso garante decis√µes fundamentadas sobre qual representa√ß√£o ret√©m melhor a sem√¢ntica e a estrutura relevante do texto.\n",
        "\n",
        "---\n",
        "\n",
        "### üóÇÔ∏è **T√©cnicas implementadas**\n",
        "\n",
        "A seguir, o pipeline aplica **cinco estrat√©gias de vetoriza√ß√£o**, cada uma com sua hip√≥tese de valor para o contexto dos textos de reclama√ß√µes:\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ **A. Bag of Words (BoW)**  \n",
        "- **Descri√ß√£o:** Representa o texto por contagem de palavras.  \n",
        "- **Hip√≥tese:** Palavras isoladas, sem ordem, podem j√° ser discriminativas para identificar categorias.  \n",
        "- **Limita√ß√£o:** N√£o captura rela√ß√µes entre palavras ou significado contextual.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ **B. TF-IDF (Unigrama)**  \n",
        "- **Descri√ß√£o:** Similar ao BoW, mas pondera cada palavra pelo seu peso informativo, penalizando termos muito comuns.  \n",
        "- **Hip√≥tese:** Palavras raras podem carregar mais valor preditivo.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ **C. TF-IDF (Uni + Bi + Tri-gramas)**  \n",
        "- **Descri√ß√£o:** Extende o TF-IDF para considerar pares e trios de palavras consecutivas.  \n",
        "- **Hip√≥tese:** Express√µes compostas e pequenas frases s√£o relevantes para capturar contexto (ex.: ‚Äúcart√£o de cr√©dito‚Äù, ‚Äúsem autoriza√ß√£o pr√©via‚Äù).\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ **D. Word2Vec (CBOW e Skip-Gram)**  \n",
        "- **Descri√ß√£o:** Usa embeddings pr√©-treinados NILC (`cbow_s300.txt` e `skip_s300.txt`), calculando a m√©dia vetorial dos tokens lematizados.  \n",
        "- **Hip√≥tese:** Representa similaridade sem√¢ntica entre palavras, agrupando contextos similares, mesmo com varia√ß√µes de vocabul√°rio.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ **E. Sentence-Transformer**  \n",
        "- **Descri√ß√£o:** Utiliza o modelo `'distiluse-base-multilingual-cased-v2'` para gerar embeddings sem√¢nticos de senten√ßas inteiras.  \n",
        "- **Hip√≥tese:** Capta rela√ß√µes mais profundas e depend√™ncias de longo alcance, superando limita√ß√µes do Word2Vec ao tratar o texto como uma unidade completa.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è **Como ser√° utilizado**\n",
        "Cada matriz vetorial gerada (A ‚Üí E) alimentar√° os mesmos algoritmos de classifica√ß√£o supervisionada (Logistic Regression, Random Forest, etc.).  \n",
        "Os resultados ser√£o comparados usando m√©tricas padronizadas (Accuracy, Precision, Recall e F1 Score Weighted).  \n",
        "Esta compara√ß√£o permitir√° selecionar a estrat√©gia de vetoriza√ß√£o com maior valor explicativo para os dados, garantindo **robustez metodol√≥gica** e **rastreabilidade dos artefatos**.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß ETAPA: VETORIZA√á√ÉO COM COUNTVECTORIZER (UNIGRAMA) + SPLIT\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Junta tokens lematizados em string\n",
        "df['texto_final'] = df['tokens_lematizados'].apply(lambda tokens: \" \".join(tokens))\n",
        "\n",
        "# Vetoriza com CountVectorizer\n",
        "vectorizer = CountVectorizer(ngram_range=(1,1))\n",
        "X = vectorizer.fit_transform(df['texto_final'])\n",
        "\n",
        "# Vetor alvo\n",
        "y = df['categoria']\n",
        "\n",
        "print(f\"Shape da matriz vetorial: {X.shape}\")\n",
        "\n",
        "# Split estratificado\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Shape X_train: {X_train.shape} | X_test: {X_test.shape}\")\n",
        "\n",
        "# Salva vetoriza√ß√µes como matrizes esparsas se quiser persistir\n",
        "from scipy import sparse\n",
        "\n",
        "sparse.save_npz(BASE_DIR / 'X_train_countvec.npz', X_train)\n",
        "sparse.save_npz(BASE_DIR / 'X_test_countvec.npz', X_test)\n",
        "y_train.to_csv(BASE_DIR / 'y_train_countvec.csv', index=False)\n",
        "y_test.to_csv(BASE_DIR / 'y_test_countvec.csv', index=False)\n",
        "\n",
        "print(\"\\n‚úÖ Vetoriza√ß√£o CountVectorizer conclu√≠da e salva!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß ETAPA: VETORIZA√á√ÉO COM TFIDFVECTORIZER (UNI + BI) + SPLIT\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy import sparse\n",
        "\n",
        "# Junta tokens lematizados novamente se necess√°rio\n",
        "df['texto_final'] = df['tokens_lematizados'].apply(lambda tokens: \" \".join(tokens))\n",
        "\n",
        "# Vetoriza com TFIDF (unigrama + bigrama)\n",
        "vectorizer_tfidf = TfidfVectorizer(ngram_range=(1,2))\n",
        "X = vectorizer_tfidf.fit_transform(df['texto_final'])\n",
        "\n",
        "# Vetor alvo\n",
        "y = df['categoria']\n",
        "\n",
        "print(f\"Shape da matriz vetorial TF-IDF: {X.shape}\")\n",
        "\n",
        "# Split estratificado\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Shape X_train: {X_train.shape} | X_test: {X_test.shape}\")\n",
        "\n",
        "# Salva vetoriza√ß√£o\n",
        "sparse.save_npz(BASE_DIR / 'X_train_tfidf_uni_bi.npz', X_train)\n",
        "sparse.save_npz(BASE_DIR / 'X_test_tfidf_uni_bi.npz', X_test)\n",
        "y_train.to_csv(BASE_DIR / 'y_train_tfidf_uni_bi.csv', index=False)\n",
        "y_test.to_csv(BASE_DIR / 'y_test_tfidf_uni_bi.csv', index=False)\n",
        "\n",
        "print(\"\\n‚úÖ Vetoriza√ß√£o TfidfVectorizer (Uni + Bi) conclu√≠da e salva!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß ETAPA: VETORIZA√á√ÉO COM TFIDFVECTORIZER (UNI + BI + TRI) + SPLIT\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy import sparse\n",
        "\n",
        "# Junta tokens lematizados se necess√°rio\n",
        "df['texto_final'] = df['tokens_lematizados'].apply(lambda tokens: \" \".join(tokens))\n",
        "\n",
        "# Vetoriza com TFIDF (uni + bi + tri)\n",
        "vectorizer_tfidf_tri = TfidfVectorizer(ngram_range=(1,3))\n",
        "X = vectorizer_tfidf_tri.fit_transform(df['texto_final'])\n",
        "\n",
        "# Vetor alvo\n",
        "y = df['categoria']\n",
        "\n",
        "print(f\"Shape da matriz vetorial TF-IDF (Uni + Bi + Tri): {X.shape}\")\n",
        "\n",
        "# Split estratificado\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Shape X_train: {X_train.shape} | X_test: {X_test.shape}\")\n",
        "\n",
        "# Salva vetoriza√ß√£o\n",
        "sparse.save_npz(BASE_DIR / 'X_train_tfidf_tri.npz', X_train)\n",
        "sparse.save_npz(BASE_DIR / 'X_test_tfidf_tri.npz', X_test)\n",
        "y_train.to_csv(BASE_DIR / 'y_train_tfidf_tri.csv', index=False)\n",
        "y_test.to_csv(BASE_DIR / 'y_test_tfidf_tri.csv', index=False)\n",
        "\n",
        "print(\"\\n‚úÖ Vetoriza√ß√£o TfidfVectorizer (Uni + Bi + Tri) conclu√≠da e salva!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Recomenda√ß√£o: fa√ßa o download e carregue previamente os arquivos txt cbow_s300 e skip_s300 no diret√≥rio de sua utiliza√ß√£o antes de continuar\n",
        "\n",
        "Reposit√≥rio Original (para ambos CBOW e Skip-gram)\n",
        "\n",
        "O reposit√≥rio original para Word Embeddings Pr√©-treinados em Portugu√™s √© o do NILC, acess√≠vel em http://nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc\n",
        "\n",
        "Reposit√≥rio do Professor (links diretos para download dos arquivos compactados)\n",
        "\n",
        "Para o modelo CBOW, o arquivo zip sugerido est√° em https://dados-ml-pln.s3-sa-east-1.amazonaws.com/cbow_s300.zip.  \n",
        "Ap√≥s o download, o arquivo cbow_s300.txt √© descompactado para uso\n",
        "\n",
        "Para o modelo Skip-gram, o arquivo zip sugerido est√° em https://dados-ml-pln.s3-sa-east-1.amazonaws.com/skip_s300.zip.\n",
        "De forma semelhante, o arquivo skip_s300.txt √© extra√≠do para utiliza√ß√£o\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß PASSO 1: INSTALA√á√ÉO DAS VERS√ïES RECOMENDADAS PARA Word2Vec\n",
        "\n",
        "!pip install gensim==4.3.2 scipy==1.10.1 numpy==1.23.5 --quiet\n",
        "\n",
        "print(\"‚úÖ Depend√™ncias fixas instaladas: gensim==4.3.2 | scipy==1.10.1 | numpy==1.23.5\")\n",
        "\n",
        "# üìå IMPORTANTE: Reinicie o ambiente agora!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß ETAPA: WORD2VEC CBOW \n",
        "\n",
        "# üì¶ Importa√ß√µes necess√°rias\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# üü¢ Ativa barra de progresso para loops demorados\n",
        "tqdm.pandas()\n",
        "\n",
        "# üìÇ Define BASE_DIR e cria se n√£o existir\n",
        "BASE_DIR = Path('/content/drive/MyDrive/MBA_NLP/bases_criadas')\n",
        "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"‚úÖ Diret√≥rio BASE_DIR garantido: {BASE_DIR}\")\n",
        "\n",
        "# üìÇ Define modelos_dir e valida\n",
        "modelos_dir = Path('/content/drive/MyDrive/MBA_NLP/modelos')\n",
        "assert modelos_dir.exists(), f\"‚ùå Diret√≥rio {modelos_dir} n√£o existe. Verifique o Drive.\"\n",
        "\n",
        "# ‚úÖ Carrega DataFrame se necess√°rio\n",
        "df_path = BASE_DIR / 'dados_tokens_lematizados.csv'\n",
        "assert df_path.exists(), f\"‚ùå Arquivo {df_path} n√£o encontrado. Gere-o antes de prosseguir.\"\n",
        "\n",
        "df = pd.read_csv(df_path)\n",
        "print(f\"‚úÖ DataFrame carregado de: {df_path}\")\n",
        "\n",
        "# ‚öôÔ∏è Converte string para lista de tokens se necess√°rio\n",
        "if isinstance(df['tokens_lematizados'].iloc[0], str):\n",
        "    df['tokens_lematizados'] = df['tokens_lematizados'].apply(eval)\n",
        "\n",
        "print(df.head(20))\n",
        "\n",
        "# ‚úÖ Carrega o modelo CBOW\n",
        "w2v_cbow_path = modelos_dir / 'cbow_s300.txt'\n",
        "assert w2v_cbow_path.exists(), f\"‚ùå Modelo CBOW n√£o encontrado em {w2v_cbow_path}\"\n",
        "\n",
        "w2v_cbow = KeyedVectors.load_word2vec_format(str(w2v_cbow_path), binary=False)\n",
        "w2v_dim = w2v_cbow.vector_size\n",
        "print(f\"‚úÖ Modelo CBOW carregado | Dimens√£o dos embeddings: {w2v_dim}\")\n",
        "\n",
        "# ‚öôÔ∏è Fun√ß√£o de m√©dia vetorial\n",
        "def get_mean_vector(tokens):\n",
        "    vectors = [w2v_cbow[word] for word in tokens if word in w2v_cbow]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(w2v_dim)\n",
        "\n",
        "# üîÑ Vetoriza√ß√£o com barra de progresso\n",
        "X_cbow = np.vstack(df['tokens_lematizados'].progress_apply(get_mean_vector))\n",
        "print(f\"‚úÖ Shape da matriz CBOW: {X_cbow.shape}\")\n",
        "\n",
        "# üéØ Target\n",
        "y = df['categoria']\n",
        "\n",
        "# üîÄ Split estratificado\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_cbow, y, test_size=0.25, stratify=y, random_state=42\n",
        ")\n",
        "print(f\"‚úÖ Split conclu√≠do: X_train {X_train.shape}, X_test {X_test.shape}\")\n",
        "\n",
        "# üíæ Salva vetores e r√≥tulos\n",
        "np.save(BASE_DIR / 'X_train_word2vec_cbow.npy', X_train)\n",
        "np.save(BASE_DIR / 'X_test_word2vec_cbow.npy', X_test)\n",
        "y_train.to_csv(BASE_DIR / 'y_train_word2vec_cbow.csv', index=False)\n",
        "y_test.to_csv(BASE_DIR / 'y_test_word2vec_cbow.csv', index=False)\n",
        "\n",
        "print(\"‚úÖ Vetoriza√ß√£o CBOW conclu√≠da, artefatos salvos com rastreabilidade.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß ETAPA: WORD2VEC SKIP-GRAM \n",
        "\n",
        "# üì¶ Importa√ß√µes\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "# üìÇ BASE_DIR\n",
        "BASE_DIR = Path('/content/drive/MyDrive/MBA_NLP/bases_criadas')\n",
        "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"‚úÖ Diret√≥rio BASE_DIR garantido: {BASE_DIR}\")\n",
        "\n",
        "# üìÇ Modelos\n",
        "modelos_dir = Path('/content/drive/MyDrive/MBA_NLP/modelos')\n",
        "assert modelos_dir.exists(), f\"‚ùå Diret√≥rio {modelos_dir} n√£o existe. Verifique o Drive.\"\n",
        "\n",
        "# ‚úÖ Carrega DataFrame se necess√°rio\n",
        "df_path = BASE_DIR / 'dados_tokens_lematizados.csv'\n",
        "assert df_path.exists(), f\"‚ùå Arquivo {df_path} n√£o encontrado. Gere-o antes de prosseguir.\"\n",
        "\n",
        "df = pd.read_csv(df_path)\n",
        "print(f\"‚úÖ DataFrame carregado de: {df_path}\")\n",
        "\n",
        "# ‚öôÔ∏è Converte string para lista se necess√°rio\n",
        "if isinstance(df['tokens_lematizados'].iloc[0], str):\n",
        "    df['tokens_lematizados'] = df['tokens_lematizados'].apply(eval)\n",
        "\n",
        "print(df.head(20))\n",
        "\n",
        "# ‚úÖ Carrega modelo Skip-Gram\n",
        "w2v_skip_path = modelos_dir / 'skip_s300.txt'\n",
        "assert w2v_skip_path.exists(), f\"‚ùå Modelo Skip-Gram n√£o encontrado em {w2v_skip_path}\"\n",
        "\n",
        "w2v_skip = KeyedVectors.load_word2vec_format(str(w2v_skip_path), binary=False)\n",
        "w2v_dim = w2v_skip.vector_size\n",
        "print(f\"‚úÖ Skip-Gram carregado | Dimens√£o dos embeddings: {w2v_dim}\")\n",
        "\n",
        "# ‚öôÔ∏è Fun√ß√£o de m√©dia vetorial\n",
        "def get_mean_vector_skip(tokens):\n",
        "    vectors = [w2v_skip[word] for word in tokens if word in w2v_skip]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(w2v_dim)\n",
        "\n",
        "# üîÑ Vetoriza√ß√£o com barra de progresso\n",
        "X_skip = np.vstack(df['tokens_lematizados'].progress_apply(get_mean_vector_skip))\n",
        "print(f\"‚úÖ Shape da matriz Skip-Gram: {X_skip.shape}\")\n",
        "\n",
        "# üéØ Target\n",
        "y = df['categoria']\n",
        "\n",
        "# üîÄ Split estratificado\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_skip, y, test_size=0.25, stratify=y, random_state=42\n",
        ")\n",
        "print(f\"‚úÖ Split conclu√≠do: X_train {X_train.shape}, X_test {X_test.shape}\")\n",
        "\n",
        "# üíæ Salva vetores e r√≥tulos\n",
        "np.save(BASE_DIR / 'X_train_word2vec_skip.npy', X_train)\n",
        "np.save(BASE_DIR / 'X_test_word2vec_skip.npy', X_test)\n",
        "y_train.to_csv(BASE_DIR / 'y_train_word2vec_skip.csv', index=False)\n",
        "y_test.to_csv(BASE_DIR / 'y_test_word2vec_skip.csv', index=False)\n",
        "\n",
        "print(\"‚úÖ Vetoriza√ß√£o Skip-Gram conclu√≠da, artefatos salvos com rastreabilidade.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß ETAPA: Sentence Transformers\n",
        "\n",
        "# REINICIAR O AMBIENTE AGORA!\n",
        "\n",
        "# ‚úÖ Reinstala compat√≠vel para embeddings ST\n",
        "!pip install -U numpy==1.26.4 sentence-transformers==3.2.1 transformers==4.46.3 --quiet\n",
        "\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# üì¶ Monta Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# üìÇ Diret√≥rio BASE\n",
        "BASE_DIR = Path('/content/drive/MyDrive/MBA_NLP/bases_criadas')\n",
        "print(f\"‚úÖ Diret√≥rio de bases: {BASE_DIR}\")\n",
        "\n",
        "# ‚úÖ Carrega e normaliza df\n",
        "df = pd.read_csv(BASE_DIR / 'dados_tokens_lematizados.csv')\n",
        "df['tokens_lematizados'] = df['tokens_lematizados'].apply(eval)\n",
        "\n",
        "import unidecode\n",
        "\n",
        "def normalize_tokens(tokens):\n",
        "    text = \" \".join(tokens)\n",
        "    text = text.lower()\n",
        "    text = unidecode.unidecode(text)\n",
        "    return text\n",
        "\n",
        "df['texto_final'] = df['tokens_lematizados'].apply(normalize_tokens)\n",
        "print(df[['tokens_lematizados', 'texto_final']].head(10))\n",
        "\n",
        "# ‚úÖ Carrega modelo\n",
        "model_st = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')\n",
        "\n",
        "embeddings = model_st.encode(df['texto_final'].tolist(), show_progress_bar=True)\n",
        "X = np.array(embeddings)\n",
        "print(f\"‚úÖ Embeddings shape: {X.shape}\")\n",
        "\n",
        "y = df['categoria']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "np.save(BASE_DIR / 'X_train_sentence_transformer.npy', X_train)\n",
        "np.save(BASE_DIR / 'X_test_sentence_transformer.npy', X_test)\n",
        "y_train.to_csv(BASE_DIR / 'y_train_sentence_transformer.csv', index=False)\n",
        "y_test.to_csv(BASE_DIR / 'y_test_sentence_transformer.csv', index=False)\n",
        "\n",
        "print(\"‚úÖ Sentence-Transformer embeddings gerados e salvos.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìä Bloco 4 ‚Äî Classifica√ß√£o e Avalia√ß√£o Supervisionada\n",
        "\n",
        "### üéØ **Prop√≥sito**\n",
        "Esta etapa compara **todas as estrat√©gias de vetoriza√ß√£o** (A‚ÄìE), geradas no Macro-Bloco 3, usando os **mesmos conjuntos de treino e teste**.  \n",
        "A meta √© avaliar qual t√©cnica gera a melhor performance preditiva na categoriza√ß√£o de reclama√ß√µes, com base em **Accuracy, Precision, Recall e F1 Score Weighted**, visando F1 ‚â• 75%.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è **Decis√µes T√©cnicas**\n",
        "- **Modelos:** Logistic Regression (baseline) e Random Forest (n√£o linear) para cada abordagem.\n",
        "- **Carregamento:** Diferencia matrizes esparsas (`CountVectorizer`, `TF-IDF`) e densas (`Word2Vec`, `Sentence-Transformer`).\n",
        "- **Loop rastre√°vel:** Usa prints claros para indicar formato (`SPARSE` ou `DENSE`), forma da matriz e m√©tricas parciais.\n",
        "- **Sa√≠da:** Relat√≥rio `relatorio_comparativo_classificacao.csv` salvo em `/MBA_NLP/bases_criadas` para auditoria e compara√ß√µes futuras.\n",
        "\n",
        "---\n",
        "\n",
        "### üìë **Resultado esperado**\n",
        "Ao final, voc√™ ter√° uma tabela padronizada com todos os resultados supervisionados, validando qual feature engineering sustenta melhor performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß BLOCO 4 ‚Äî CLASSIFICA√á√ÉO & AVALIA√á√ÉO ‚Äî PROTOCOLO LLM V5.2\n",
        "\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from scipy import sparse\n",
        "\n",
        "# üìÇ 1Ô∏è‚É£ Base de sa√≠da garantida\n",
        "BASE_DIR = Path('/content/drive/MyDrive/MBA_NLP/bases_criadas')\n",
        "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"‚úÖ Diret√≥rio de bases: {BASE_DIR}\")\n",
        "\n",
        "# üìã 2Ô∏è‚É£ Lista de vetores dispon√≠veis\n",
        "vetores = {\n",
        "    \"bow\": (\"X_train_countvec.npz\", \"X_test_countvec.npz\", \"y_train_countvec.csv\", \"y_test_countvec.csv\"),\n",
        "    \"tfidf_uni\": (\"X_train_tfidf_uni_bi.npz\", \"X_test_tfidf_uni_bi.npz\", \"y_train_tfidf_uni_bi.csv\", \"y_test_tfidf_uni_bi.csv\"),\n",
        "    \"tfidf_tri\": (\"X_train_tfidf_tri.npz\", \"X_test_tfidf_tri.npz\", \"y_train_tfidf_tri.csv\", \"y_test_tfidf_tri.csv\"),\n",
        "    \"word2vec_cbow\": (\"X_train_word2vec_cbow.npy\", \"X_test_word2vec_cbow.npy\", \"y_train_word2vec_cbow.csv\", \"y_test_word2vec_cbow.csv\"),\n",
        "    \"word2vec_skip\": (\"X_train_word2vec_skip.npy\", \"X_test_word2vec_skip.npy\", \"y_train_word2vec_skip.csv\", \"y_test_word2vec_skip.csv\"),\n",
        "    \"sentence_transformer\": (\"X_train_sentence_transformer.npy\", \"X_test_sentence_transformer.npy\", \"y_train_sentence_transformer.csv\", \"y_test_sentence_transformer.csv\"),\n",
        "}\n",
        "\n",
        "# üìä 3Ô∏è‚É£ DataFrame de resultados\n",
        "resultados = []\n",
        "\n",
        "# üîÅ 4Ô∏è‚É£ Loop para cada abordagem\n",
        "for name, (X_train_file, X_test_file, y_train_file, y_test_file) in vetores.items():\n",
        "\n",
        "    # ‚öôÔ∏è Carrega vetores SPARSE ou DENSE\n",
        "    if \"countvec\" in X_train_file or \"tfidf\" in X_train_file:\n",
        "        X_train = sparse.load_npz(BASE_DIR / X_train_file)\n",
        "        X_test = sparse.load_npz(BASE_DIR / X_test_file)\n",
        "        print(f\"‚úÖ {name.upper()} carregado como SPARSE: {X_train.shape}\")\n",
        "    else:\n",
        "        X_train = np.load(BASE_DIR / X_train_file)\n",
        "        X_test = np.load(BASE_DIR / X_test_file)\n",
        "        print(f\"‚úÖ {name.upper()} carregado como DENSE: {X_train.shape}\")\n",
        "\n",
        "    # üéØ Target labels\n",
        "    y_train = pd.read_csv(BASE_DIR / y_train_file).squeeze()\n",
        "    y_test = pd.read_csv(BASE_DIR / y_test_file).squeeze()\n",
        "\n",
        "    # üöÄ Modelos supervisonados\n",
        "    models = {\n",
        "        \"LogisticRegression\": LogisticRegression(max_iter=1000),\n",
        "        \"RandomForest\": RandomForestClassifier(n_estimators=100)\n",
        "    }\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "        rec = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "        resultados.append({\n",
        "            \"Vetorizacao\": name,\n",
        "            \"Modelo\": model_name,\n",
        "            \"Accuracy\": round(acc, 4),\n",
        "            \"Precision\": round(prec, 4),\n",
        "            \"Recall\": round(rec, 4),\n",
        "            \"F1_Score_Weighted\": round(f1, 4)\n",
        "        })\n",
        "\n",
        "        print(f\"‚úÖ [{name.upper()}] {model_name} | F1 Score Weighted: {f1:.4f}\")\n",
        "\n",
        "# üíæ 5Ô∏è‚É£ Relat√≥rio final\n",
        "df_resultados = pd.DataFrame(resultados)\n",
        "df_resultados.to_csv(BASE_DIR / \"relatorio_comparativo_classificacao.csv\", index=False)\n",
        "print(f\"\\n‚úÖ Relat√≥rio comparativo salvo em: {BASE_DIR / 'relatorio_comparativo_classificacao.csv'}\")\n",
        "\n",
        "df_resultados\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìä Relat√≥rio Comparativo ‚Äî Vetoriza√ß√£o & Classifica√ß√£o\n",
        "\n",
        "‚úÖ **Diret√≥rio de bases:** `/content/drive/MyDrive/MBA_NLP/bases_criadas`\n",
        "\n",
        "| Abordagem                    | Shape             | F1 Score Weighted |\n",
        "|------------------------------|-------------------|-------------------|\n",
        "| **BoW** LogisticRegression   | SPARSE (15804, 36496) | **0.9005** |\n",
        "| BoW RandomForest             | SPARSE (15804, 36496) | 0.8048 |\n",
        "| TFIDF_UNI LogisticRegression | SPARSE (15804, 779791) | 0.8931 |\n",
        "| TFIDF_UNI RandomForest       | SPARSE (15804, 779791) | 0.8085 |\n",
        "| TFIDF_TRI LogisticRegression | SPARSE (15804, 2,552,999) | 0.8787 |\n",
        "| TFIDF_TRI RandomForest       | SPARSE (15804, 2,552,999) | 0.7794 |\n",
        "| Word2Vec CBOW LogisticRegression | DENSE (15804, 300) | 0.8007 |\n",
        "| Word2Vec CBOW RandomForest   | DENSE (15804, 300) | 0.7119 |\n",
        "| Word2Vec Skip LogisticRegression | DENSE (15804, 300) | 0.8121 |\n",
        "| Word2Vec Skip RandomForest   | DENSE (15804, 300) | 0.7278 |\n",
        "| Sentence-Transformer LogisticRegression | DENSE (14750, 512) | 0.7722 |\n",
        "| Sentence-Transformer RandomForest     | DENSE (14750, 512) | 0.7098 |\n",
        "\n",
        "‚úÖ **Relat√≥rio comparativo salvo em:**  \n",
        "`/content/drive/MyDrive/MBA_NLP/bases_criadas/relatorio_comparativo_classificacao.csv`\n",
        "\n",
        "---\n",
        "\n",
        "### üìå **Resumo**\n",
        "\n",
        "- O baseline escolhido √© **BoW + LogisticRegression**, com **F1 Weighted de 0.9005**, superando a meta de 0.75.\n",
        "- Todas as demais abordagens ficam documentadas para auditoria, compara√ß√£o futura e experimenta√ß√£o avan√ßada.\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "---\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68SiMjcWqD_m"
      },
      "source": [
        "###**Valida√ß√£o do professor**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T24EasckqG2I"
      },
      "source": [
        "Consolidar apenas os scripts do seu **modelo campe√£o**, desde o carregamento do dataframe, separa√ß√£o das amostras, tratamentos utilizados (fun√ß√µes, limpezas, etc.), cria√ß√£o dos objetos de vetoriza√ß√£o dos textos e modelo treinado e outras implementa√ß√µes utilizadas no processo de desenvolvimento do modelo.\n",
        "\n",
        "O modelo precisar atingir um score na m√©trica F1 Score superior a 75%.\n",
        "\n",
        "**Aten√ß√£o:**\n",
        "- **Implemente aqui apenas os scripts que fazem parte do modelo campe√£o.**\n",
        "- **Execute o pipeline do modelo campe√£o completamente para garantir que n√£o tet√° erros no script.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFA-CYfawkEJ"
      },
      "source": [
        "---\n",
        "---\n",
        "O pipeline completo do modelo campe√£o, que usa Bag of Words (BoW), foi reescrito com o objetivo de consolidar tudo em blocos autocontidos, organizados e execut√°veis em qualquer ambiente.\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuJtvcfXo3J4"
      },
      "source": [
        "## Etapa 0 ‚Äî Setup de Depend√™ncias e Bibliotecas\n",
        "\n",
        "Antes de executar o pipeline do *modelo campe√£o*, garantimos que todas as bibliotecas e recursos estejam dispon√≠veis.  \n",
        "Este bloco prepara o ambiente, faz downloads necess√°rios (`nltk` e `SpaCy`), carrega o modelo de linguagem em portugu√™s e combina as listas de stopwords `nltk` + `SpaCy`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting click (from nltk)\n",
            "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting joblib (from nltk)\n",
            "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting regex>=2021.8.3 (from nltk)\n",
            "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "Collecting tqdm (from nltk)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.2.1-py3-none-any.whl (102 kB)\n",
            "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
            "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
            "Successfully installed click-8.2.1 joblib-1.5.1 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.8.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
            "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
            "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
            "  Downloading murmurhash-1.0.13-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
            "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
            "  Downloading cymem-2.0.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\n",
            "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
            "  Downloading preshed-3.0.10-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
            "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
            "  Downloading thinc-8.3.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
            "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
            "  Downloading srsly-2.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
            "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
            "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
            "  Downloading typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/wrm/venv/lib/python3.12/site-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /home/wrm/venv/lib/python3.12/site-packages (from spacy) (2.3.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/wrm/venv/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
            "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
            "Requirement already satisfied: jinja2 in /home/wrm/venv/lib/python3.12/site-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /home/wrm/venv/lib/python3.12/site-packages (from spacy) (79.0.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/wrm/venv/lib/python3.12/site-packages (from spacy) (25.0)\n",
            "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
            "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
            "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.33.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /home/wrm/venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
            "Collecting typing-inspection>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/wrm/venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/wrm/venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/wrm/venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/wrm/venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Downloading blis-1.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: click>=8.0.0 in /home/wrm/venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Downloading cloudpathlib-0.21.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Downloading smart_open-7.3.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/wrm/venv/lib/python3.12/site-packages (from jinja2->spacy) (3.0.2)\n",
            "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
            "  Downloading marisa_trie-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/wrm/venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Downloading spacy-3.8.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m33.9/33.9 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
            "Downloading cymem-2.0.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (227 kB)\n",
            "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
            "Downloading murmurhash-1.0.13-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (128 kB)\n",
            "Downloading preshed-3.0.10-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (869 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m869.3/869.3 kB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
            "Downloading pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
            "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
            "Downloading srsly-2.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.3.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typer-0.16.0-py3-none-any.whl (46 kB)\n",
            "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
            "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
            "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading blis-1.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cloudpathlib-0.21.1-py3-none-any.whl (52 kB)\n",
            "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
            "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
            "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Downloading smart_open-7.3.0-py3-none-any.whl (61 kB)\n",
            "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
            "Downloading marisa_trie-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89 kB)\n",
            "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: cymem, wrapt, wasabi, typing-inspection, spacy-loggers, spacy-legacy, shellingham, pydantic-core, murmurhash, mdurl, marisa-trie, cloudpathlib, catalogue, blis, annotated-types, srsly, smart-open, pydantic, preshed, markdown-it-py, language-data, rich, langcodes, confection, typer, thinc, weasel, spacy\n",
            "Successfully installed annotated-types-0.7.0 blis-1.3.0 catalogue-2.0.10 cloudpathlib-0.21.1 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.13 preshed-3.0.10 pydantic-2.11.7 pydantic-core-2.33.2 rich-14.0.0 shellingham-1.5.4 smart-open-7.3.0 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.6 typer-0.16.0 typing-inspection-0.4.1 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.2\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /home/wrm/venv/lib/python3.12/site-packages (from scikit-learn) (2.3.1)\n",
            "Collecting scipy>=1.8.0 (from scikit-learn)\n",
            "  Downloading scipy-1.16.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /home/wrm/venv/lib/python3.12/site-packages (from scikit-learn) (1.5.1)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading scikit_learn-1.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.16.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m35.1/35.1 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
            "Successfully installed scikit-learn-1.7.0 scipy-1.16.0 threadpoolctl-3.6.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /home/wrm/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pt-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.8.0/pt_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pt-core-news-sm\n",
            "Successfully installed pt-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_sm')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Todas as depend√™ncias foram verificadas e carregadas.\n",
            "Stopwords combinadas: 500 termos\n"
          ]
        }
      ],
      "source": [
        "# üîß ETAPA: SETUP DE DEPEND√äNCIAS E BIBLIOTECAS\n",
        "\n",
        "# Instalar pandas se necess√°rio\n",
        "try:\n",
        "    import pandas as pd\n",
        "except ImportError:\n",
        "    !pip install pandas\n",
        "    import pandas as pd\n",
        "\n",
        "# Instalar numpy se necess√°rio\n",
        "try:\n",
        "    import numpy as np\n",
        "except ImportError:\n",
        "    !pip install numpy\n",
        "    import numpy as np\n",
        "\n",
        "# M√≥dulos padr√£o da biblioteca padr√£o Python (n√£o precisam de instala√ß√£o)\n",
        "import string\n",
        "import re\n",
        "import os\n",
        "\n",
        "# Instalar unidecode se necess√°rio\n",
        "try:\n",
        "    import unidecode\n",
        "except ImportError:\n",
        "    !pip install unidecode\n",
        "    import unidecode\n",
        "\n",
        "# Instalar nltk se necess√°rio\n",
        "try:\n",
        "    import nltk\n",
        "except ImportError:\n",
        "    !pip install nltk\n",
        "    import nltk\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Instalar spacy se necess√°rio\n",
        "try:\n",
        "    import spacy\n",
        "except ImportError:\n",
        "    !pip install -U spacy\n",
        "    import spacy\n",
        "\n",
        "# Instalar sklearn se necess√°rio\n",
        "try:\n",
        "    import sklearn\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.feature_extraction.text import CountVectorizer\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
        "except ImportError:\n",
        "    !pip install scikit-learn\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.feature_extraction.text import CountVectorizer\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
        "\n",
        "# Instalar tqdm se necess√°rio\n",
        "try:\n",
        "    from tqdm.notebook import tqdm\n",
        "except ImportError:\n",
        "    !pip install tqdm\n",
        "    from tqdm.notebook import tqdm\n",
        "\n",
        "# Instalar IPython.display se necess√°rio (normalmente vem com Jupyter)\n",
        "try:\n",
        "    from IPython.display import display\n",
        "except ImportError:\n",
        "    !pip install IPython\n",
        "    from IPython.display import display\n",
        "\n",
        "# Baixar stopwords PT do nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Baixar e carregar SpaCy pt_core_news_sm\n",
        "try:\n",
        "    nlp = spacy.load('pt_core_news_sm')\n",
        "except:\n",
        "    import subprocess\n",
        "    subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"pt_core_news_sm\"])\n",
        "    nlp = spacy.load('pt_core_news_sm')\n",
        "\n",
        "# Combinar stopwords NLTK + SpaCy\n",
        "stopwords_pt = set(stopwords.words('portuguese'))\n",
        "stopwords_spacy = nlp.Defaults.stop_words\n",
        "combined_stopwords = stopwords_pt.union(stopwords_spacy)\n",
        "\n",
        "print(f\"‚úÖ Todas as depend√™ncias foram verificadas e carregadas.\")\n",
        "print(f\"Stopwords combinadas: {len(combined_stopwords)} termos\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ClM-JTJo3FK"
      },
      "source": [
        "## Etapa 1 ‚Äî Pipeline do Modelo Campe√£o com Lematiza√ß√£o, Placeholders e Vetoriza√ß√£o BoW\n",
        "\n",
        "Este bloco executa o pipeline do *modelo campe√£o* de forma **port√°vel** e **rastre√°vel**, incluindo:\n",
        "- **Carregamento do dataset** pela URL oficial,\n",
        "- **Pr√©-processamento** com substitui√ß√£o inteligente de placeholders (`<DATE>`, `<NUMBER>`, `<PII>`),\n",
        "- **Tokeniza√ß√£o e lematiza√ß√£o** com SpaCy,\n",
        "- **Remo√ß√£o de stopwords combinadas** (`nltk` + `SpaCy`),\n",
        "- **Barra de progresso `tqdm`** para monitorar o avan√ßo da lematiza√ß√£o,\n",
        "- **Montagem do texto final** para vetoriza√ß√£o,\n",
        "- Vetoriza√ß√£o com **CountVectorizer (unigrama)**,\n",
        "- **Divis√£o treino/teste** estratificada com `random_state=42`,\n",
        "- Treinamento do **LogisticRegression**,\n",
        "- Avalia√ß√£o com **classification_report**, **F1 Score weighted** e matriz de confus√£o.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   id_reclamacao              data_abertura  \\\n",
            "0        3229299  2019-05-01T12:00:00-05:00   \n",
            "1        3199379  2019-04-02T12:00:00-05:00   \n",
            "2        3233499  2019-05-06T12:00:00-05:00   \n",
            "3        3180294  2019-03-14T12:00:00-05:00   \n",
            "4        3224980  2019-04-27T12:00:00-05:00   \n",
            "\n",
            "                             categoria  \\\n",
            "0              Hipotecas / Empr√©stimos   \n",
            "1  Cart√£o de cr√©dito / Cart√£o pr√©-pago   \n",
            "2  Cart√£o de cr√©dito / Cart√£o pr√©-pago   \n",
            "3  Cart√£o de cr√©dito / Cart√£o pr√©-pago   \n",
            "4           Servi√ßos de conta banc√°ria   \n",
            "\n",
            "                                descricao_reclamacao  \n",
            "0  Bom dia, meu nome √© xxxx xxxx e agrade√ßo se vo...  \n",
            "1  Atualizei meu cart√£o xxxx xxxx em xx/xx/2018 e...  \n",
            "2  O cart√£o Chase foi relatado em xx/xx/2019. No ...  \n",
            "3  Em xx/xx/2018, enquanto tentava reservar um ti...  \n",
            "4  Meu neto me d√™ cheque por {$ 1600,00} Eu depos...  \n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "077d16bf481e4a05859aeef1a2e5583d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/21072 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                descricao_reclamacao  \\\n",
            "0  Bom dia, meu nome √© xxxx xxxx e agrade√ßo se vo...   \n",
            "1  Atualizei meu cart√£o xxxx xxxx em xx/xx/2018 e...   \n",
            "2  O cart√£o Chase foi relatado em xx/xx/2019. No ...   \n",
            "3  Em xx/xx/2018, enquanto tentava reservar um ti...   \n",
            "4  Meu neto me d√™ cheque por {$ 1600,00} Eu depos...   \n",
            "\n",
            "                                  tokens_lematizados  \n",
            "0  [dia, nome, pii, pii, agradeco, voce, puder, a...  \n",
            "1  [atualizei, cartao, pii, pii, informar, por o,...  \n",
            "2  [cartao, chase, relatar, em o, entanto, pedido...  \n",
            "3  [reservar, ticket, pii, pii, deparar, oferta, ...  \n",
            "4  [neto, cheque, depositei, em o, conta, chase, ...  \n",
            "Shape matriz vetorial: (21072, 29483)\n",
            "X_train: (15804, 29483), X_test: (5268, 29483)\n",
            "\n",
            "üîé **Classification Report**:\n",
            "\n",
            "                                     precision    recall  f1-score   support\n",
            "\n",
            "Cart√£o de cr√©dito / Cart√£o pr√©-pago       0.91      0.92      0.91      1252\n",
            "            Hipotecas / Empr√©stimos       0.91      0.91      0.91       962\n",
            "                             Outros       0.87      0.84      0.86       558\n",
            "       Roubo / Relat√≥rio de disputa       0.88      0.88      0.88      1206\n",
            "         Servi√ßos de conta banc√°ria       0.91      0.92      0.91      1290\n",
            "\n",
            "                           accuracy                           0.90      5268\n",
            "                          macro avg       0.90      0.89      0.89      5268\n",
            "                       weighted avg       0.90      0.90      0.90      5268\n",
            "\n",
            "\n",
            "‚úÖ **F1 Score (weighted): 89.91%**\n",
            "\n",
            "üîç **Matriz de Confus√£o:**\n"
          ]
        },
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Cart√£o de cr√©dito / Cart√£o pr√©-pago",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "Hipotecas / Empr√©stimos",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "Outros",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "Roubo / Relat√≥rio de disputa",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "Servi√ßos de conta banc√°ria",
                  "rawType": "int64",
                  "type": "integer"
                }
              ],
              "ref": "6f38d3b8-694a-41e3-b6b4-cf8a96aa3089",
              "rows": [
                [
                  "Cart√£o de cr√©dito / Cart√£o pr√©-pago",
                  "1149",
                  "20",
                  "21",
                  "48",
                  "14"
                ],
                [
                  "Hipotecas / Empr√©stimos",
                  "16",
                  "880",
                  "14",
                  "29",
                  "23"
                ],
                [
                  "Outros",
                  "22",
                  "23",
                  "469",
                  "17",
                  "27"
                ],
                [
                  "Roubo / Relat√≥rio de disputa",
                  "46",
                  "30",
                  "19",
                  "1058",
                  "53"
                ],
                [
                  "Servi√ßos de conta banc√°ria",
                  "28",
                  "15",
                  "15",
                  "51",
                  "1181"
                ]
              ],
              "shape": {
                "columns": 5,
                "rows": 5
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cart√£o de cr√©dito / Cart√£o pr√©-pago</th>\n",
              "      <th>Hipotecas / Empr√©stimos</th>\n",
              "      <th>Outros</th>\n",
              "      <th>Roubo / Relat√≥rio de disputa</th>\n",
              "      <th>Servi√ßos de conta banc√°ria</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Cart√£o de cr√©dito / Cart√£o pr√©-pago</th>\n",
              "      <td>1149</td>\n",
              "      <td>20</td>\n",
              "      <td>21</td>\n",
              "      <td>48</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Hipotecas / Empr√©stimos</th>\n",
              "      <td>16</td>\n",
              "      <td>880</td>\n",
              "      <td>14</td>\n",
              "      <td>29</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Outros</th>\n",
              "      <td>22</td>\n",
              "      <td>23</td>\n",
              "      <td>469</td>\n",
              "      <td>17</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Roubo / Relat√≥rio de disputa</th>\n",
              "      <td>46</td>\n",
              "      <td>30</td>\n",
              "      <td>19</td>\n",
              "      <td>1058</td>\n",
              "      <td>53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Servi√ßos de conta banc√°ria</th>\n",
              "      <td>28</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>51</td>\n",
              "      <td>1181</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     Cart√£o de cr√©dito / Cart√£o pr√©-pago  \\\n",
              "Cart√£o de cr√©dito / Cart√£o pr√©-pago                                 1149   \n",
              "Hipotecas / Empr√©stimos                                               16   \n",
              "Outros                                                                22   \n",
              "Roubo / Relat√≥rio de disputa                                          46   \n",
              "Servi√ßos de conta banc√°ria                                            28   \n",
              "\n",
              "                                     Hipotecas / Empr√©stimos  Outros  \\\n",
              "Cart√£o de cr√©dito / Cart√£o pr√©-pago                       20      21   \n",
              "Hipotecas / Empr√©stimos                                  880      14   \n",
              "Outros                                                    23     469   \n",
              "Roubo / Relat√≥rio de disputa                              30      19   \n",
              "Servi√ßos de conta banc√°ria                                15      15   \n",
              "\n",
              "                                     Roubo / Relat√≥rio de disputa  \\\n",
              "Cart√£o de cr√©dito / Cart√£o pr√©-pago                            48   \n",
              "Hipotecas / Empr√©stimos                                        29   \n",
              "Outros                                                         17   \n",
              "Roubo / Relat√≥rio de disputa                                 1058   \n",
              "Servi√ßos de conta banc√°ria                                     51   \n",
              "\n",
              "                                     Servi√ßos de conta banc√°ria  \n",
              "Cart√£o de cr√©dito / Cart√£o pr√©-pago                          14  \n",
              "Hipotecas / Empr√©stimos                                      23  \n",
              "Outros                                                       27  \n",
              "Roubo / Relat√≥rio de disputa                                 53  \n",
              "Servi√ßos de conta banc√°ria                                 1181  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# üîß ETAPA: PIPELINE COMPLETO DO MODELO **CAMPE√ÉO** \n",
        "# 1Ô∏è‚É£ Carregar dataset\n",
        "url = \"https://dados-ml-pln.s3.sa-east-1.amazonaws.com/tickets_reclamacoes_classificados.csv\"\n",
        "df = pd.read_csv(url, sep=';')\n",
        "print(df.head(5))\n",
        "\n",
        "# 2Ô∏è‚É£ Limpeza b√°sica\n",
        "df.dropna(subset=['descricao_reclamacao'], inplace=True)\n",
        "\n",
        "# 3Ô∏è‚É£ Substitui√ß√£o inteligente de placeholders\n",
        "def replace_placeholders(text):\n",
        "    text = re.sub(r'\\b\\d{2}/\\d{2}/\\d{4}\\b', '<DATE>', text)\n",
        "    text = re.sub(r'\\b\\d{2}-\\d{2}-\\d{4}\\b', '<DATE>', text)\n",
        "    text = re.sub(r'\\b\\d{4}\\b', '<YEAR>', text)\n",
        "    text = re.sub(r'\\b\\d+\\b', '<NUMBER>', text)\n",
        "    text = re.sub(r'X{2,}', '<PII>', text, flags=re.IGNORECASE)\n",
        "    return text\n",
        "\n",
        "# 4Ô∏è‚É£ Fun√ß√£o de pr√©-processamento + lematiza√ß√£o\n",
        "def preprocess_and_lemmatize(text):\n",
        "    text = replace_placeholders(text)\n",
        "    text = text.lower()\n",
        "    text = unidecode.unidecode(text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    doc = nlp(text)\n",
        "    tokens = [\n",
        "        token.lemma_ for token in doc \n",
        "        if token.is_alpha and token.lemma_ not in combined_stopwords\n",
        "    ]\n",
        "    return tokens\n",
        "\n",
        "# 5Ô∏è‚É£ Usar tqdm para progresso\n",
        "from tqdm.notebook import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "df['tokens_lematizados'] = df['descricao_reclamacao'].progress_apply(preprocess_and_lemmatize)\n",
        "\n",
        "print(df[['descricao_reclamacao', 'tokens_lematizados']].head(5))\n",
        "\n",
        "# 6Ô∏è‚É£ Texto final para vetoriza√ß√£o\n",
        "df['texto_final'] = df['tokens_lematizados'].apply(lambda tokens: \" \".join(tokens))\n",
        "\n",
        "# 7Ô∏è‚É£ Vetoriza√ß√£o BoW\n",
        "vectorizer = CountVectorizer(ngram_range=(1,1))\n",
        "X = vectorizer.fit_transform(df['texto_final'])\n",
        "y = df['categoria']\n",
        "\n",
        "print(f\"Shape matriz vetorial: {X.shape}\")\n",
        "\n",
        "# 8Ô∏è‚É£ Split treino/teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, stratify=y, random_state=42\n",
        ")\n",
        "print(f\"X_train: {X_train.shape}, X_test: {X_test.shape}\")\n",
        "\n",
        "# 9Ô∏è‚É£ Treinar modelo\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# üîü Avaliar com apresenta√ß√£o formatada\n",
        "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Report\n",
        "report = classification_report(y_test, y_pred, target_names=clf.classes_, digits=2)\n",
        "print(\"\\nüîé **Classification Report**:\\n\")\n",
        "print(report)\n",
        "\n",
        "# F1 Score weighted\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "print(f\"\\n‚úÖ **F1 Score (weighted): {f1:.2%}**\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "labels = clf.classes_\n",
        "cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
        "\n",
        "print(\"\\nüîç **Matriz de Confus√£o:**\")\n",
        "display(cm_df)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
